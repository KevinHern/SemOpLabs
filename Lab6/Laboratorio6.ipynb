{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpyv7HR0D42IHvLY42tMix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHern/SemOpLabs/blob/master/Lab6/Laboratorio6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ruRTBviEee9",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eehKm8Q58tO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only works in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkLv28P-Ejqs",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xBhjDW4El7n",
        "colab_type": "text"
      },
      "source": [
        "## Authorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P22PyVBp8-a-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df08cd62-7271-4443-9671-3dd2fc121fc6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HfK5B_E5ph",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pkgVxpYFVRs",
        "colab_type": "text"
      },
      "source": [
        "## Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Y96z8SFXtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "\n",
        "    print(\"\\nShape dimensions:\\n\")\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        print(np.array(loaded).shape)\n",
        "        loaded.append(data)\n",
        "      \n",
        "     \n",
        "    # stack group so that features are the 3rd dimension\n",
        "    print(np.array(loaded).shape)\n",
        "    loaded = np.dstack(loaded)\n",
        "    print(np.array(loaded).shape)\n",
        "    return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "    # load all train\n",
        "    trainX, trainy = load_dataset_group('train', prefix + 'UCI HAR Dataset/')\n",
        "    # load all test\n",
        "    testX, testy = load_dataset_group('test', prefix + 'UCI HAR Dataset/')\n",
        "    # zero-offset class values\n",
        "    trainy = trainy - 1\n",
        "    testy = testy - 1\n",
        "    print(\"\\nOne Hot Encoding: \\n\",trainy)\n",
        "    # one hot encode y\n",
        "    trainy = to_categorical(trainy)\n",
        "    testy = to_categorical(testy)\n",
        "    print(\"\\n\",trainy)\n",
        "    return trainX, trainy, testX, testy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gquXUHJgFYWi",
        "colab_type": "text"
      },
      "source": [
        "## Downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyiL6FVq9TIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b86dd249-c4c0-4b49-d970-d0a57b3f3e3a"
      },
      "source": [
        "dataset_path = \"/content/drive/My Drive/SP1_2020/lab6/\"\n",
        "\n",
        "# run this only once to save dataset to drive\n",
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/HAR_Smartphones.zip -P \"/content/drive/My Drive/SP1_2020/lab6/\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 02:36:00--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/HAR_Smartphones.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/SP1_2020/lab6/HAR_Smartphones.zip.1’\n",
            "\n",
            "HAR_Smartphones.zip 100%[===================>]  58.17M  54.7MB/s    in 1.1s    \n",
            "\n",
            "2020-03-02 02:36:02 (54.7 MB/s) - ‘/content/drive/My Drive/SP1_2020/lab6/HAR_Smartphones.zip.1’ saved [60999314/60999314]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfIkKx2C9Vis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "896b0d63-2588-4463-d2b6-d8ba3cfb4ced"
      },
      "source": [
        "!unzip -q \"/content/drive/My Drive/SP1_2020/lab6/HAR_Smartphones.zip\" -d \"/content/drive/My Drive/SP1_2020/lab6/\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace /content/drive/My Drive/SP1_2020/lab6/UCI HAR Dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/__MACOSX/UCI HAR Dataset/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/UCI HAR Dataset/activity_labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/__MACOSX/UCI HAR Dataset/._activity_labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/UCI HAR Dataset/features.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/__MACOSX/UCI HAR Dataset/._features.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/UCI HAR Dataset/features_info.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace /content/drive/My Drive/SP1_2020/lab6/UCI HAR Dataset/features_info.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVjFyujyFtGj",
        "colab_type": "text"
      },
      "source": [
        "## Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3KzBQeKFulm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "5114fa3f-c945-42f7-e238-f2d2a9be53b3"
      },
      "source": [
        "trainX, trainy, testX, testy = load_dataset(dataset_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape dimensions:\n",
            "\n",
            "(0,)\n",
            "(1, 7352, 128)\n",
            "(2, 7352, 128)\n",
            "(3, 7352, 128)\n",
            "(4, 7352, 128)\n",
            "(5, 7352, 128)\n",
            "(6, 7352, 128)\n",
            "(7, 7352, 128)\n",
            "(8, 7352, 128)\n",
            "(9, 7352, 128)\n",
            "(7352, 128, 9)\n",
            "\n",
            "Shape dimensions:\n",
            "\n",
            "(0,)\n",
            "(1, 2947, 128)\n",
            "(2, 2947, 128)\n",
            "(3, 2947, 128)\n",
            "(4, 2947, 128)\n",
            "(5, 2947, 128)\n",
            "(6, 2947, 128)\n",
            "(7, 2947, 128)\n",
            "(8, 2947, 128)\n",
            "(9, 2947, 128)\n",
            "(2947, 128, 9)\n",
            "\n",
            "One Hot Encoding: \n",
            " [[4]\n",
            " [4]\n",
            " [4]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5wy6dgmFEGA",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1BrU5mFFfQ",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqEZAQV09kX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "85ad9dff-6be2-4015-daec-102cdcb2cc4a"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz5JOgagFGiN",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT-nsXM79uIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "# checkpoint\n",
        "filepath = dataset_path + \"output.best.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min',save_freq='epoch')\n",
        "\n",
        "# tensorboard\n",
        "#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_callback = TensorBoard(logdir, histogram_freq=1,profile_batch=0)\n",
        "\n",
        "# callbacks\n",
        "#callbacks_list = [checkpoint_callback,tensorboard_callback]\n",
        "callbacks_list = [checkpoint_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkPCIw2FKXN",
        "colab_type": "text"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMDbzPtY-PFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "    global model\n",
        "    verbose, epochs, batch_size = 1, 10, 32\n",
        "    samples, n_timesteps, n_features, n_outputs = trainX.shape[0], trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(64, activation='relu', input_shape=(n_timesteps, n_features))))\n",
        "    #model.add(Bidirectional(LSTM(16)))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,callbacks=callbacks_list)\n",
        "    # evaluate model\n",
        "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4PkXmVOFw55",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I4JgPxJGHkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "0826addf-0887-4a18-c275-5be0ec2babb8"
      },
      "source": [
        "score = evaluate_model(trainX, trainy, testX, testy)\n",
        "score = score * 100.0\n",
        "print('>Accuracy: %.3f' % (score))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7352 samples\n",
            "Epoch 1/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 52081140.9090 - accuracy: 0.6164\n",
            "Epoch 00001: loss improved from inf to 51911126.30662, saving model to /content/drive/My Drive/SP1_2020/lab6/output.best.h5\n",
            "7352/7352 [==============================] - 29s 4ms/sample - loss: 51911126.3066 - accuracy: 0.6163\n",
            "Epoch 2/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.8466 - accuracy: 0.6921\n",
            "Epoch 00002: loss improved from 51911126.30662 to 0.84664, saving model to /content/drive/My Drive/SP1_2020/lab6/output.best.h5\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: 0.8466 - accuracy: 0.6921\n",
            "Epoch 3/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1691\n",
            "Epoch 00003: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1692\n",
            "Epoch 4/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1668\n",
            "Epoch 00004: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 26s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 5/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1672\n",
            "Epoch 00005: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 6/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1666\n",
            "Epoch 00006: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 26s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 7/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1669\n",
            "Epoch 00007: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 8/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1662\n",
            "Epoch 00008: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 9/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1670\n",
            "Epoch 00009: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            "Epoch 10/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.1666\n",
            "Epoch 00010: loss did not improve from 0.84664\n",
            "7352/7352 [==============================] - 27s 4ms/sample - loss: nan - accuracy: 0.1668\n",
            ">Accuracy: 16.831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFqIxu7kL-do",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tBa6oz3M4zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5407fe2c-78de-48aa-e32f-b9402b5bd56c"
      },
      "source": [
        "t = testX[5]\n",
        "t = t.reshape(1,128,9)\n",
        "\n",
        "\n",
        "print(\"\\nclass:\",model.predict_classes(t))\n",
        "print(\"\\nprobs:\",model.predict_proba(t))\n",
        "\n",
        "#print(np.argmax(result))\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "class: [0]\n",
            "\n",
            "probs: [[nan nan nan nan nan nan]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}