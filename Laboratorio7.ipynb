{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHern/SemOpLabs/blob/master/Laboratorio7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcblzuK0DR11",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRhsFZ9v7scS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only works in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzORkJiDApa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89D2AFBp9HIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRS62OtYEGeR",
        "colab_type": "text"
      },
      "source": [
        "# Model Autoencoder Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjFkvFd-AqcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvAutoencoder:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, filters=(32, 64 ,128), latentDim=32):\n",
        "        # initialize the input shape to be \"channels last\" along with\n",
        "        # the channels dimension itself\n",
        "        # channels dimension itself\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # define the input to the encoder\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = inputs\n",
        "\n",
        "        # loop over the number of filters\n",
        "        for f in filters:\n",
        "            # apply a CONV => RELU => BN operation\n",
        "            x = Conv2D(f, (1, 1), strides=1, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # flatten the network and then construct our latent vector\n",
        "        volumeSize = K.int_shape(x)\n",
        "        print(\"volumeSize:\",volumeSize)\n",
        "        x = Flatten()(x)\n",
        "        print(\"x shape\", K.int_shape(x))\n",
        "        latent = Dense(latentDim)(x)\n",
        "\n",
        "        # build the encoder model\n",
        "        encoder = Model(inputs, latent, name=\"encoder\")\n",
        "\n",
        "        # start building the decoder model which will accept the\n",
        "        # output of the encoder as its inputs\n",
        "        latentInputs = Input(shape=(latentDim,))\n",
        "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "        print(\"prod shape:\",np.prod(volumeSize[1:]))\n",
        "        print(\"x shape\",K.int_shape(x))\n",
        "        print(volumeSize)\n",
        "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "        print(\"x shape\",K.int_shape(x))\n",
        "\n",
        "        # loop over our number of filters again, but this time in\n",
        "        # reverse order\n",
        "        for f in filters[::-1]:\n",
        "            # apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "            x = Conv2DTranspose(f, (1, 1), strides=1,\n",
        "                padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # apply a single CONV_TRANSPOSE layer used to recover the\n",
        "        # original depth of the image\n",
        "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "        outputs = Activation(\"sigmoid\")(x)\n",
        "\n",
        "        # build the decoder model\n",
        "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
        "\n",
        "        # our autoencoder is the encoder + decoder\n",
        "        autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
        "            name=\"autoencoder\")\n",
        "\n",
        "        # return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "        return (encoder, decoder, autoencoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkMGsGxfE9EP",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCwvkQKCFGCP",
        "colab_type": "text"
      },
      "source": [
        "## Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKpO3AW6-ELO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_unsupervised_dataset(data, labels, validLabel=4,\n",
        "    anomalyLabel=5, contam=0.01, seed=42):\n",
        "    # grab all indexes of the supplied class label that are *truly*\n",
        "    # that particular label, then grab the indexes of the image\n",
        "    # labels that will serve as our \"anomalies\"\n",
        "    validIdxs = np.where(labels == validLabel)[0]\n",
        "    anomalyIdxs = np.where(labels == anomalyLabel)[0]\n",
        "\n",
        "    # randomly shuffle both sets of indexes\n",
        "    random.shuffle(validIdxs)\n",
        "    random.shuffle(anomalyIdxs)\n",
        "\n",
        "    # compute the total number of anomaly data points to select\n",
        "    i = int(len(validIdxs) * contam)\n",
        "    anomalyIdxs = anomalyIdxs[:i]\n",
        "\n",
        "    # use NumPy array indexing to extract both the valid images and\n",
        "    # \"anomlay\" images\n",
        "    validImages = data[validIdxs]\n",
        "    anomalyImages = data[anomalyIdxs]\n",
        "\n",
        "    # stack the valid images and anomaly images together to form a\n",
        "    # single data matrix and then shuffle the rows\n",
        "    images = np.vstack([validImages, anomalyImages])\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(images)\n",
        "\n",
        "    # return the set of images\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUWYKH3R-HUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_predictions(decoded, gt, samples=10):\n",
        "    # initialize our list of output images\n",
        "    outputs = None\n",
        "\n",
        "    # loop over our number of output samples\n",
        "    for i in range(0, samples):\n",
        "        # grab the original image and reconstructed image\n",
        "        original = (gt[i] * 255).astype(\"uint8\")\n",
        "        recon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\n",
        "        # stack the original and reconstructed image side-by-side\n",
        "        output = np.hstack([original, recon])\n",
        "\n",
        "        # if the outputs array is empty, initialize it as the current\n",
        "        # side-by-side image display\n",
        "        if outputs is None:\n",
        "            outputs = output\n",
        "\n",
        "        # otherwise, vertically stack the outputs\n",
        "        else:\n",
        "            outputs = np.vstack([outputs, output])\n",
        "\n",
        "    # return the output images\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YuonUZdFKwZ",
        "colab_type": "text"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1aN81bQJQ_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6b9de208-6b38-4f2b-92fb-3ab467c9a5fc"
      },
      "source": [
        "# load the Fashion MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
        "\n",
        "# build our unsupervised dataset of images with a small amount of\n",
        "# contamination (i.e., anomalies) added into it\n",
        "print(\"[INFO] creating unsupervised dataset...\")\n",
        "images = build_unsupervised_dataset(trainX, trainY, validLabel=4,\n",
        "    anomalyLabel=5, contam=0.01)\n",
        "\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "images = np.expand_dims(images, axis=-1)\n",
        "images = images.astype(\"float32\") / 255.0"
      ],
      "execution_count": 528,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading MNIST dataset...\n",
            "[INFO] creating unsupervised dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhcXLNH3J33a",
        "colab_type": "text"
      },
      "source": [
        "## Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQ6Re9c-Y5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct the training and testing split\n",
        "(trainX, testX) = train_test_split(images, test_size=0.2,\n",
        "    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnqB9QdxKho5",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78i187EZKt7N",
        "colab_type": "text"
      },
      "source": [
        "## Macro Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq_XhnA_KvtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 20\n",
        "INIT_LR = 1e-3\n",
        "BS = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyDvcp9IKw0G",
        "colab_type": "text"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqUn950WKyXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "c3ad8901-8a28-4086-bec5-ace7db28e05a"
      },
      "source": [
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)"
      ],
      "execution_count": 531,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] building autoencoder...\n",
            "volumeSize: (None, 28, 28, 128)\n",
            "x shape (None, 100352)\n",
            "prod shape: 100352\n",
            "x shape (None, 100352)\n",
            "(None, 28, 28, 128)\n",
            "x shape (None, 28, 28, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MTk03mzK7Uu",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hrB9dtGa6k-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adf31cd5-cd77-4622-91b5-b5486eed283b"
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": 532,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4848, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 532
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171zIwWBgmVC",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6q2_o3gnkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stop training model when accuracy is bigger than 0.9\n",
        "class Callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('loss')<0.035):\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "callbacks = Callback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqdJR8PyK8ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9ad8aec5-e94d-48a1-e844-8b5420e5ac28"
      },
      "source": [
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit(\n",
        "    trainX, trainX,\n",
        "    validation_data=(testX, testX),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BS, callbacks = [callbacks])"
      ],
      "execution_count": 536,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4848 samples, validate on 1212 samples\n",
            "Epoch 1/20\n",
            "4848/4848 [==============================] - 56s 12ms/sample - loss: 0.0289 - val_loss: 0.1321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1FsU0ZlK-vQ",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3UcWAf-yIx",
        "colab_type": "code",
        "outputId": "0d4b863a-c7e0-42d9-caba-9ccc8a82b254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# use the convolutional autoencoder to make predictions on the\n",
        "# testing images, construct the visualization, and then save it\n",
        "# to disk\n",
        "print(\"[INFO] making predictions...\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "vis = visualize_predictions(decoded, testX)\n",
        "cv2.imwrite(\"/content/drive/My Drive/SP1_2020/autoencoder/encoder_vis.png\", vis)"
      ],
      "execution_count": 537,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] making predictions...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 537
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2L6JeLgJKme",
        "colab_type": "code",
        "outputId": "2dfc69e6-5e99-4ab7-ee97-6bebf87613ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "img = cv2.imread('/content/drive/My Drive/SP1_2020/autoencoder/encoder_vis.png', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 538,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-538-cd173fba9a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/SP1_2020/autoencoder/encoder_vis.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKecfCn0_b2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "372a8468-39a6-4840-b20c-e0c4053083d5"
      },
      "source": [
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"/content/drive/My Drive/SP1_2020/autoencoder/plot.png\")"
      ],
      "execution_count": 539,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-539-29251713b8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (1,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHFtvBiH9Co",
        "colab_type": "code",
        "outputId": "28e7c98c-2822-4a19-a43a-f64cb1ec0df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "img = cv2.imread('/content/drive/My Drive/SP1_2020/autoencoder/plot.png', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 540,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-540-0f44b0ab00dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/SP1_2020/autoencoder/plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q7QfK9z71Jr",
        "colab_type": "code",
        "outputId": "da23766e-283b-4ce9-df13-73dfd7685391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# serialize the image data to disk\n",
        "print(\"[INFO] saving image data...\")\n",
        "f = open(\"/content/drive/My Drive/SP1_2020/autoencoder/output/images.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(images))\n",
        "f.close()\n",
        "\n",
        "# serialize the autoencoder model to disk\n",
        "print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder.save(\"/content/drive/My Drive/SP1_2020/autoencoder/output/autoencoder.model\", save_format=\"h5\")"
      ],
      "execution_count": 541,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] saving image data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-541-3c78d1f9031a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] saving image data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/SP1_2020/autoencoder/output/images.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/SP1_2020/autoencoder/output/images.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEdXVLGIGNge",
        "colab_type": "text"
      },
      "source": [
        "##**FIND ANOMALIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTVIyGlmG4TC",
        "colab_type": "code",
        "outputId": "962f254a-de6b-4d48-c2d7-2cf60d32fa8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "#import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "# load the model and image data from disk\n",
        "print(\"[INFO] loading autoencoder and image data...\")\n",
        "autoencoder = load_model(\"/content/drive/My Drive/SP1_2020/autoencoder/output/autoencoder.model\")\n",
        "images = pickle.loads(open(\"/content/drive/My Drive/SP1_2020/autoencoder/output/images.pickle\", \"rb\").read())"
      ],
      "execution_count": 542,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading autoencoder and image data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-542-644c56a76607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load the model and image data from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading autoencoder and image data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/SP1_2020/autoencoder/output/autoencoder.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/SP1_2020/autoencoder/output/images.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/drive/My Drive/SP1_2020/autoencoder/output/autoencoder.model/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P60sfSsG8XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions on our image data and initialize our list of\n",
        "# reconstruction errors\n",
        "decoded = autoencoder.predict(images)\n",
        "errors = []\n",
        "\n",
        "# loop over all original images and their corresponding\n",
        "# reconstructions\n",
        "for (image, recon) in zip(images, decoded):\n",
        "    # compute the mean squared error between the ground-truth image\n",
        "    # and the reconstructed image, then add it to our list of errors\n",
        "    mse = np.mean((image - recon) ** 2)\n",
        "    errors.append(mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBCWx8G2HBtx",
        "colab_type": "code",
        "outputId": "8b068ba2-fd3d-4cab-9108-cc7b79a4aadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh = np.quantile(errors, 0.999)\n",
        "idxs = np.where(np.array(errors) >= thresh)[0]\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs)))\n",
        "\n",
        "# initialize the outputs array\n",
        "outputs = None"
      ],
      "execution_count": 544,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] mse threshold: 0.24988616344332687\n",
            "[INFO] 7 outliers found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4USdkEm6-Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop over the indexes of images with a high mean squared error term\n",
        "for i in idxs:\n",
        "    # grab the original image and reconstructed image\n",
        "    original = (images[i] * 255).astype(\"uint8\")\n",
        "    recon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\n",
        "    # stack the original and reconstructed image side-by-side\n",
        "    output = np.hstack([original, recon])\n",
        "\n",
        "    # if the outputs array is empty, initialize it as the current\n",
        "    # side-by-side image display\n",
        "    if outputs is None:\n",
        "        outputs = output\n",
        "\n",
        "    # otherwise, vertically stack the outputs\n",
        "    else:\n",
        "        outputs = np.vstack([outputs, output])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Owd5B7HtzO",
        "colab_type": "code",
        "outputId": "9ec07192-684e-43fd-c5f6-798c2cd8762c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "# show the output visualization\n",
        "cv2_imshow(outputs)"
      ],
      "execution_count": 546,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAADECAIAAABFtHUrAAAwdUlEQVR4nJWdaYxkVfn/z11rX7un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\n6B/hw3Ex5z2FYbht27b/B7Uv+MlR6Do1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=56x196 at 0x7F51F8A51630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}